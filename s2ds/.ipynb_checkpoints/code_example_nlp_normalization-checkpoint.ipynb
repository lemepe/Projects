{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import pandas_gbq\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import cld2\n",
    "\n",
    "#set max rows and columns\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigquery setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']=os.path.expanduser('PATH_TO_USER_CREDENTIALS')\n",
    "\n",
    "#Initialize bigquery service\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_info(df):\n",
    "    from IPython.display import display\n",
    "    print(\"The shape of the dataframe is:{df_shape}\".format(df_shape=df.shape))\n",
    "    print(\"The types of the columns in the dataframe are below:\")\n",
    "    display(df.dtypes)\n",
    "    print(\"The head of the dataframe is below:\")\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get master table for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = client.query(\"SELECT * FROM zendesk_s2ds_clean.merged_t_te_1st_row_lj_cust_only;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_results_raw = query_job.result().to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info(master_results_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create copy of body column with raw content: te_ce_body_raw\n",
    "master_results_raw['te_ce_body_raw'] = master_results_raw['te_ce_body']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info(master_results_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_results_raw['te_ce_body_raw'].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_results_raw['te_ce_body_raw'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language_for_each_row(df, name_of_column_to_be_detected):\n",
    "    \n",
    "    ''' this function detect the language of specific row item and return df with new column \"language\" '''\n",
    "    \n",
    "    df_to_be_returned = df.copy()\n",
    "    \n",
    "    list_of_language_details = []\n",
    "    list_of_top_language = []\n",
    "    list_of_percentage = []\n",
    "    \n",
    "    for index, row in df[name_of_column_to_be_detected].iteritems(): \n",
    "    \n",
    "        try:\n",
    "            if len(row) > 0:        \n",
    "                isReliable, textBytesFound, details  = cld2.detect(row)\n",
    "                list_of_language_details.append(details)\n",
    "                list_of_top_language.append(details[0].language_name)\n",
    "                list_of_percentage .append(details[0].percent)\n",
    "\n",
    "                #df.loc[index, 'language'] = details\n",
    "                \n",
    "            else:\n",
    "                list_of_language_details.append('failed')\n",
    "                list_of_top_language.append('failed')\n",
    "                list_of_percentage.append('failed')\n",
    "                \n",
    "        except:\n",
    "            list_of_language_details.append('empty_body')\n",
    "            list_of_top_language.append('empty_body')\n",
    "            list_of_percentage.append('empty_body')\n",
    "          \n",
    "                     \n",
    "    df_to_be_returned['language_detail']  =   list_of_language_details\n",
    "    df_to_be_returned['top_language']  =   list_of_top_language  \n",
    "    df_to_be_returned['language_percent']  =  list_of_percentage  \n",
    "    \n",
    "    return df_to_be_returned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_results_wc = master_results_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_results_wc = detect_language_for_each_row(master_results_wc,\"te_ce_body_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info(master_results_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_results_wc = master_results_wc.drop([\"language_detail\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_results_wc['language_percent'] = master_results_wc['language_percent'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_results_wc['top_language'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "dataset_ref = client.dataset('zendesk_s2ds_clean')\n",
    "table_ref = dataset_ref.table('merged_t_te_1st_row_lj_cust_only_lang')\n",
    "client.delete_table(table_ref, not_found_ok=True)\n",
    "client.load_table_from_dataframe(master_results_wc, table_ref).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get only English emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = client.query(\"SELECT * from zendesk_s2ds_clean.merged_t_te_1st_row_lj_cust_only_lang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_results_lang = query_job.result().to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_email_body_and_language_only_EN = master_results_lang[master_results_lang.top_language == 'ENGLISH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_with_email_body_and_language_only_EN.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_email_body_and_language_only_EN['top_language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info(df_with_email_body_and_language_only_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_with_email_body_and_language_only_EN.language_percent.astype(int).hist(cumulative=True, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlim([60,100])\n",
    "df_with_email_body_and_language_only_EN.language_percent.astype(int).hist(cumulative=True, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlim([80,100])\n",
    "df_with_email_body_and_language_only_EN.language_percent.astype(int).hist(cumulative=True, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlim([90,100])\n",
    "df_with_email_body_and_language_only_EN.language_percent.astype(int).hist(cumulative=True, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_with_email_body_and_language_only_EN[df_with_email_body_and_language_only_EN.language_percent.astype(int) > 95].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_email_body_and_language_only_EN[df_with_email_body_and_language_only_EN.language_percent.astype(int) > 90].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_email_body_and_language_only_EN[df_with_email_body_and_language_only_EN.language_percent.astype(int) > 80].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_email_body_and_language_only_EN[df_with_email_body_and_language_only_EN.language_percent.astype(int) > 75].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_email_body_and_language_only_EN[df_with_email_body_and_language_only_EN.language_percent.astype(int) > 70].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_email_body_and_language_only_EN[df_with_email_body_and_language_only_EN.language_percent.astype(int) > 95].shape[0]/df_with_email_body_and_language_only_EN.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_email_body_and_language_only_EN[df_with_email_body_and_language_only_EN.language_percent.astype(int) > 95].shape[0]/df_with_email_body_and_language_only_EN.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above distribution, it looks like the majority of emails (93%) classified as English can be classified as English even with a strict confidence cut-off of 95% - so we will take only emails that we are 95% sure are in English forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_email_body_and_language_only_EN_0_95_cutoff = df_with_email_body_and_language_only_EN[df_with_email_body_and_language_only_EN.language_percent.astype(int) > 95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_email_body_and_language_only_EN_0_95_cutoff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_email_body_and_language_only_EN_0_95_cutoff.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanitize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of characters in emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_with_email_body_and_language_only_EN_0_95_cutoff.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_set = set()\n",
    "index_count = 0\n",
    "for i in range(1000, len(df_with_email_body_and_language_only_EN_0_95_cutoff.te_ce_body), 1000):\n",
    "#for i in range(1000, 5000, 1000):\n",
    "    original_i = i - 1000\n",
    "    set_letters = set(df_with_email_body_and_language_only_EN_0_95_cutoff.te_ce_body[original_i:i].apply(list).sum())\n",
    "    final_set = final_set.union(set_letters)\n",
    "    print(original_i, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(list(final_set)))\n",
    "print(index_count)\n",
    "print(final_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sorted set of characters in our emails are:\n",
    "\n",
    "```\n",
    "['\\t', '\\r', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\xa0', 'Â¡', 'Â¢', 'Â£', 'Â©', 'Â«', '\\xad', 'Â®', 'Â°', 'Â²', 'Â´', 'Â·', 'Â»', 'Â¿', 'Ã€', 'Ã', 'Ã‚', 'Ã„', 'Ã…', 'Ã‰', 'Ã', 'Ã–', 'Ã—', 'Ã˜', 'Ãœ', 'ÃŸ', 'Ã ', 'Ã¡', 'Ã¢', 'Ã¤', 'Ã¥', 'Ã§', 'Ã¨', 'Ã©', 'Ãª', 'Ã¬', 'Ã­', 'Ã®', 'Ã±', 'Ã³', 'Ã´', 'Ã¶', 'Ã¸', 'Ã¹', 'Ãº', 'Ã»', 'Ã¼', 'Ã¾', 'Ä°', 'Å‚', 'Å‹', 'Å ', 'Å¾', 'Ç€', 'É•', 'Éª', 'Ê’', 'Ëˆ', 'Ì€', 'Ì', 'Î”', 'Î ', 'Î¬', 'Î­', 'Î±', 'Îµ', 'Î·', 'Î¹', 'Îº', 'Î¼', 'Î¿', 'Ï', 'Ï‚', 'Ïƒ', 'Ï„', 'Ð¡', '×‘', '×”', '×ž', '×¥', '×¨', '×³', '\\u200b', '\\u200c', '\\u200d', '\\u200e', 'â€', 'â€‘', 'â€“', 'â€”', 'â€˜', 'â€™', 'â€œ', 'â€', 'â€ž', 'â€¢', 'â€¦', '\\u2028', '\\u202c', '\\u202d', '\\u2063', 'â‚¬', 'â‡§', 'â”‚', 'â–‡', 'â–', 'â–½', 'â—', 'â˜…', 'â˜†', 'â˜Ž', 'â˜˜', 'â˜¹', 'â˜º', 'â™¥', 'âœŒ', 'âœ¨', 'â¤', '\\u3000', 'ã€‚', 'ä¸Š', 'ä¸‹', 'ä¸', 'ä»¶', 'ä¿', 'å‚³', 'åˆ¶', 'åˆ', 'å«', 'å››', 'å¢ƒ', 'å¤', 'å¤´', 'å®', 'å¯„', 'æ˜Ÿ', 'æœŸ', 'æ¯’', 'æ³¢', 'æµ·', 'ç’°', 'ç”¨', 'ç”°', 'ç—…', 'ç›', 'ç¨‹', 'ç¯€', 'ç´„', 'ç´™', 'è€…', 'è­·', 'é€', 'ê²½', 'ê¸ˆ', 'ê¹€', 'ë‚ ', 'ë…„', 'ë‹ˆ', 'ë‹¤', 'ë©', 'ë³¸', 'ì„œ', 'ìˆ˜', 'ìŠµ', 'ì—', 'ì˜¤', 'ìš”', 'ì¼', 'ìžˆ', 'ì „', 'ì§€', 'ì§œ', 'í† ', 'í™”', 'í›„', 'ï¬', 'ï¸', '\\ufeff', 'ï¼Œ', 'ï¿¡', 'ï¿¼', 'ï¿½', 'ðŸŒˆ', 'ðŸŒ²', 'ðŸŒ·', 'ðŸŒ¸', 'ðŸŒ¹', 'ðŸ¼', 'ðŸ‘‹', 'ðŸ’›', 'ðŸ˜€', 'ðŸ˜‚', 'ðŸ˜ƒ', 'ðŸ˜Š', 'ðŸ˜‹', 'ðŸ˜‘', 'ðŸ˜”', 'ðŸ˜•', 'ðŸ˜–', 'ðŸ˜ž', 'ðŸ˜¢', 'ðŸ˜£', 'ðŸ˜¥', 'ðŸ˜©', 'ðŸ˜«', 'ðŸ˜¬', 'ðŸ™', 'ðŸ™‚', 'ðŸ™„', 'ðŸ™ˆ', 'ðŸ™Œ', 'ðŸ™', 'ðŸ›‹', 'ðŸ¤“', 'ðŸ¤”', 'ðŸ¤£', 'ðŸ¤¯', 'ðŸ¥º']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up the email body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Check for email ids\n",
    "pattern = '\\S+@\\S+'\n",
    "email_id_test_series = (df_with_email_body_and_language_only_EN_0_95_cutoff[df_with_email_body_and_language_only_EN_0_95_cutoff.te_ce_body.str.findall(pattern).astype(bool)].te_ce_body)\n",
    "for row in email_id_test_series:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove email-ids\n",
    "df_cutoff_clean_working_copy = df_with_email_body_and_language_only_EN_0_95_cutoff.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoff_clean_working_copy.te_ce_body = df_cutoff_clean_working_copy.te_ce_body.str.replace('\\S+@\\S+', \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "email_id_test_series = (df_cutoff_clean_working_copy[df_cutoff_clean_working_copy.te_ce_body.str.findall('\\S+@\\S+').astype(bool)].te_ce_body)\n",
    "for row in email_id_test_series:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Check for URLs\n",
    "#pattern = '[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)'\n",
    "url_test_series = (df_cutoff_clean_working_copy[df_cutoff_clean_working_copy.te_ce_body.str.findall(pattern).astype(bool)].te_ce_body)\n",
    "for row in url_test_series:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove URLs\n",
    "pattern = 'http[s]?://(?:[a-zA-Z]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoff_clean_working_copy.te_ce_body = df_cutoff_clean_working_copy.te_ce_body.str.replace(pattern, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for URLs\n",
    "pattern = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "#pattern = '[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)'\n",
    "url_test_series = (df_cutoff_clean_working_copy[df_cutoff_clean_working_copy.te_ce_body.str.findall(pattern).astype(bool)].te_ce_body)\n",
    "for row in url_test_series:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Remove {NAME} {NUMBERS}\n",
    "df_cutoff_clean_working_copy.te_ce_body = df_cutoff_clean_working_copy.te_ce_body.str.replace('{NAME}', '')\n",
    "df_cutoff_clean_working_copy.te_ce_body = df_cutoff_clean_working_copy.te_ce_body.str.replace('{NUMBER}', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Testing removing non-English characters\n",
    "list_of_things_to_keep = \"[^a-zA-Z0-9 .!?;:-_]\"\n",
    "for row in df_cutoff_clean_working_copy.te_ce_body[:5]:\n",
    "    print(row)\n",
    "for row in df_cutoff_clean_working_copy.te_ce_body.str.replace(list_of_things_to_keep,\"\")[:5]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only alphanumeric and some punctuation characters\n",
    "list_of_things_to_keep = \"[^a-zA-Z0-9 .!?;:-_]\"\n",
    "df_cutoff_clean_working_copy.te_ce_body = df_cutoff_clean_working_copy.te_ce_body.str.replace(list_of_things_to_keep,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check list of characters in dataframe after cleaning characters\n",
    "df_EN_cutoff_0_95_sanitized = df_cutoff_clean_working_copy.copy()\n",
    "final_set = set()\n",
    "index_count = 0\n",
    "for i in range(1000, len(df_EN_cutoff_0_95_sanitized.te_ce_body), 1000):\n",
    "#for i in range(1000, 5000, 1000):\n",
    "    original_i = i - 1000\n",
    "    set_letters = set(df_EN_cutoff_0_95_sanitized.te_ce_body[original_i:i].apply(list).sum())\n",
    "    final_set = final_set.union(set_letters)\n",
    "    print(original_i, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(list(final_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "dataset_ref = client.dataset('zendesk_s2ds_clean')\n",
    "table_ref = dataset_ref.table('merged_t_te_1st_row_lj_cust_only_EN_sanitized')\n",
    "client.delete_table(table_ref, not_found_ok=True)\n",
    "client.load_table_from_dataframe(df_EN_cutoff_0_95_sanitized, table_ref).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_EN_cutoff_0_95_sanitized[df_EN_cutoff_0_95_sanitized['te_ce_body'].str.contains(\"===Write\")]['te_ce_body'][92040]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_results_en_sanitized = df_EN_cutoff_0_95_sanitized.reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tags = []\n",
    "import ast\n",
    "for each_line in master_results_en_sanitized['tags'].apply(ast.literal_eval):\n",
    "    list_of_tags.extend(each_line)\n",
    "list_of_tags = list(set(list_of_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted([element for element in list_of_tags if \"_\" in element])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_results_en_sanitized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_results_en_sanitized.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_results_en_sanitized['top_language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to change data types of columns\n",
    "\n",
    "def columns_to_appropriate_types(df,col_list,dtype):\n",
    "    '''function changes types of df columns in col_list into appropriate format as defined in dtype'''\n",
    "    \n",
    "    for col in col_list:\n",
    "        df[col] = df[col].astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cols=[\"event_id\",\"ticket_id\",\"updater_id\"]\n",
    "columns_to_appropriate_types(master_results_en_sanitized,int_cols,int)\n",
    "\n",
    "str_cols=[\"event_type\",\"via\",\"tags\",\"description\",\"sys_location\",\"sys_client\",\"top_language\",\"Contact_Reason\",\"Action\",\"Product_Collection\"]\n",
    "columns_to_appropriate_types(master_results_en_sanitized,str_cols,str)\n",
    "\n",
    "master_results_en_sanitized.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all words in mail_body to lowercase\n",
    "\n",
    "master_results_en_sanitized['te_ce_body'] = master_results_en_sanitized['te_ce_body'].str.lower()\n",
    "master_results_en_sanitized['te_ce_body'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## to use: conda install -c anaconda nltk\"\n",
    "# AND\n",
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# Tokenize email body and only keep words or ? and !\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+|[?!]') #-> also remove any numbers?\n",
    "\n",
    "master_results_en_sanitized['te_ce_body_tokens'] = master_results_en_sanitized.apply(lambda row: tokenizer.tokenize(row['te_ce_body']), axis=1)\n",
    "master_results_en_sanitized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#master_results_en_sanitized['te_ce_body'].str.contains(\"te_ce_body_tokens\").value_counts()\n",
    "#master_results_en_sanitized['te_ce_body'].iloc[48333]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To USE:!conda install -c conda-forge spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO USE: \n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#spacy.load('en_core_web_sm')\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(master_results_en_sanitized['te_ce_body_tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ast\n",
    "#master_results_en_sanitized['te_ce_body_tokens'] = master_results_en_sanitized['te_ce_body_tokens'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_results_en_sanitized[\"te_ce_body_tokens_concat\"]=master_results_en_sanitized.apply(lambda row: \" \".join(row['te_ce_body_tokens']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(master_results_en_sanitized[\"te_ce_body_tokens_concat\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spacy 'en' model, apply spacy nlp function which has all lemma attributes\n",
    "#nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "nlp = en_core_web_sm.load(disable = ['parser', 'ner'])\n",
    "master_results_en_sanitized['te_ce_body_tokens_nlp']=master_results_en_sanitized.apply(lambda row: nlp(row['te_ce_body_tokens_concat']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_results_en_sanitized['te_ce_body_tokens_nlp'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep lemmattized words from SpaCy nlp function in separate row\n",
    "master_results_en_sanitized['te_ce_body_tokens_lem']=master_results_en_sanitized.apply(lambda row: [token.lemma_ for token in row['te_ce_body_tokens_nlp'] if token.lemma_], axis=1)\n",
    "master_results_en_sanitized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# particular stopwords for made\n",
    "stopwords_made =[\"made.com\", \"made\"]\n",
    "# stopwords that are in default list but we want to keep\n",
    "stopwords_not_to_be_used =[\"when\",\"where\",\"not\",\"aren\",\"aren't\",\"doesn\", \"doesn't\",\"don\",\"don't\",\n",
    "                           \"couldn't\",\"didn\", \"didn't\",\"hadn\", \"hadn't\", \"hasn\", \"hasn't\", \"haven\",\n",
    "                           \"haven't\", \"isn\", \"isn't\", 'mightn', \"mightn't\", \"mustn\", \"mustn't\", \"needn\",\n",
    "                           \"needn't\", \"shan\", \"shan't\", \"shouldn\", \"shouldn't\", \"wasn\", \"wasn't\", \"weren\",\n",
    "                           \"weren't\", \"won\", \"won't\", \"wouldn\", \"wouldn't\"]\n",
    "\n",
    "# default stopwords from package\n",
    "my_stopwords = stopwords.words('english')\n",
    "print(\"Length of default stopwords: \", len(my_stopwords))\n",
    "print(\"List of default stopwords:\", my_stopwords)\n",
    "\n",
    "# insert made specific stopwords\n",
    "my_stopwords.extend(stopwords_made)\n",
    "print(\"Length of stopwords with additional made stopwords:\", len(my_stopwords))\n",
    "print(\"List of stopwords with additional made stopwords:\", my_stopwords)\n",
    "\n",
    "# remove unwanted stopwords which were in default list\n",
    "my_stopwords = [word for word in my_stopwords if word not in stopwords_not_to_be_used]\n",
    "print(\"Length of stopwords with unwanted stopwords removed:\", len(my_stopwords))\n",
    "print(\"List of stopwords with unwanted stopwords removed:\", my_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example: remove stopwords from tokens\n",
    "word_tokens = master_results_en_sanitized['te_ce_body_tokens_lem'].iloc[0,]\n",
    "print(word_tokens)\n",
    "\n",
    "word_tokens_clean = [token for token in word_tokens if token not in my_stopwords]\n",
    "print(word_tokens_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put tokens without stopwords into a new column \n",
    "master_results_en_sanitized['te_ce_body_tokens_no_stopwords'] = master_results_en_sanitized.apply((lambda row: [token for token in row['te_ce_body_tokens'] if token not in my_stopwords]),axis=1)\n",
    "master_results_en_sanitized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put lemmatized tokens without stopwords into a new column \n",
    "master_results_en_sanitized['te_ce_body_tokens_lem_no_stopwords'] = master_results_en_sanitized.apply((lambda row: [token for token in row['te_ce_body_tokens_lem'] if token not in my_stopwords]),axis=1)\n",
    "master_results_en_sanitized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_results_en_sanitized['te_ce_body_tokens_lem_no_stopwords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_results_en_sanitized = master_results_en_sanitized.drop(\"te_ce_body_tokens_nlp\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_nlp_lists_in_df = [element for element in master_results_en_sanitized.columns if element.startswith(\"te_ce_body_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each_list in list_of_nlp_lists_in_df:\n",
    "    master_results_en_sanitized[each_list] = master_results_en_sanitized[each_list].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "dataset_ref = client.dataset('zendesk_s2ds_clean')\n",
    "table_ref = dataset_ref.table('merged_t_te_1st_row_lj_cust_only_EN_sanitized_normalized_old_reasons')\n",
    "client.delete_table(table_ref, not_found_ok=True)\n",
    "client.load_table_from_dataframe(master_results_en_sanitized, table_ref).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get proper tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_list = sorted(list(master_results_en_sanitized['Contact_Reason'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_results_en_sanitized['Contact_Reason_cleaned'] = master_results_en_sanitized['Contact_Reason'].str.replace(\"___\", \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_results_en_sanitized['Contact_Reason_cleaned'].str.split('__').str[0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_results_en_sanitized['Contact_Reason_cleaned'].str.replace('/','__').str.replace('__','_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word clouds\n",
    "## to use: conda install -c conda-forge wordcloud\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# word cloud function\n",
    "\n",
    "def show_wordcloud(df_col_text, title=None,stopwords=None):\n",
    "   ''' Show word cloud df data in text column (insert as string)'''\n",
    "   wordcloud = WordCloud(\n",
    "   background_color='white',\n",
    "   stopwords=stopwords,\n",
    "   max_words = 200,\n",
    "   max_font_size = 40,\n",
    "   scale = 3,\n",
    "   random_state = 1\n",
    "   ).generate(str(df_col_text))\n",
    "   fig = plt.figure(1, figsize=(12,12))\n",
    "   plt.axis('off')\n",
    "   if title:\n",
    "       fig.suptitle(title)\n",
    "       fig.subplots_adjust(top=2.3)\n",
    "   plt.imshow(wordcloud)\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(master_results_en_sanitized['te_ce_body_tokens_lem_no_stopwords'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud for all emails    \n",
    "    \n",
    "show_wordcloud(master_results_en_sanitized['te_ce_body_tokens_lem_no_stopwords'], \"All tags\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud_for_categories(df,col_text,col_cat,stopwords=None):\n",
    "   ''' Creates wordclouds of a text column for each category in a category column;\n",
    "   Insert column names as strings with \"\" '''\n",
    "   list_categories = list(set((df[col_cat])))\n",
    "   for list_item in list_categories:\n",
    "       cloud_mails = df[df[col_cat]==list_item]\n",
    "       show_wordcloud(cloud_mails[col_text],list_item,stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud_for_categories(master_results_en_sanitized,\"te_ce_body_tokens_lem_no_stopwords\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert list type text columns to string for uploading to bigquery\n",
    "str_cols = [col for col in master_results_en_sanitized.columns if col.startswith(\"te_ce_body\")]\n",
    "print(str_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in str_cols:   \n",
    "    master_results_en_sanitized[col] = master_results_en_sanitized[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "dataset_ref = client.dataset('zendesk_s2ds_processed')\n",
    "schema = [bigquery.SchemaField('event_id', 'INTEGER', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('ticket_id', 'INTEGER', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('event_type', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('timestamp', 'INTEGER', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('updater_id', 'INTEGER', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('via', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('created_at', 'TIMESTAMP', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('end_timestamp', 'TIMESTAMP', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('tags', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('description', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('is_public', 'BOOLEAN', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('t_created', 'TIMESTAMP', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('rel', 'INTEGER', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('te_ce_id', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('te_ce_source_rel', 'INTEGER', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('te_ce_via_source_to_address', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('te_ce_via_source_to_name', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('te_ce_via_source_from_name', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('te_ce_via_source_from_address', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('te_ce_via_channel', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('te_ce_via_reference_id', 'INTEGER', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('te_ce_type', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('te_ce_author_id', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('te_ce_body', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('te_ce_html_body', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('te_ce_public', 'BOOLEAN', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('te_ce_audit_id', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('te_ce_created_at', 'TIMESTAMP', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('te_ce_event_type', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('sys_longitude', 'FLOAT', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('sys_latitude', 'FLOAT', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('sys_location', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('sys_client', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('top_language', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('language_percent', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('tag_reason', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('tags_reason_new', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('Contact_Reason', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('Action', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('Order_Ref_Number', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('Product_Collection', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('__index_level_0__', 'INTEGER', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('te_ce_body_tokens', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('te_ce_body_tokens_lem', 'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('te_ce_body_tokens_no_stopwords',  'STRING', 'NULLABLE', None, ()),\n",
    " bigquery.SchemaField('te_ce_body_tokens_lem_no_stopwords',  'STRING', 'NULLABLE', None, ()),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_results_en_sanitized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ref = dataset_ref.table('master_table_emails_sanitized_body_normalized')\n",
    "client.delete_table(table_ref, not_found_ok=True)\n",
    "table = bigquery.Table(table_ref,schema=schema)\n",
    "client.load_table_from_dataframe(master_results_en_sanitized, table_ref).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use normalized dataset and filter for emails which have been solved with first reply\n",
    "\n",
    "- after visiting the CS team we figured that tags might be noisy as the contact reason might change because of customers replying to the same ticket with different reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add metric_set column to dataset\n",
    "query_job = client.query(\"\"\"SELECT * \n",
    "    FROM zendesk_s2ds_processed.master_table_emails_sanitized_body_normalized\n",
    "    LEFT JOIN (SELECT *, CAST(ms_ticket_id AS INT64) AS ms_t_id\n",
    "    FROM zendesk_s2ds_processed.tickets_metric_set_col_clean)\n",
    "    ON ticket_id = ms_t_id;\"\"\")\n",
    "master_results_normalized_ms = query_job.result().to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## additional column with time delta between first reply and ticket solved\n",
    "master_results_normalized_ms[\"ms_delta_time_first_reply_solved\"]=(master_results_normalized_ms['ms_full_resolution_time_in_minutes_c']) - (master_results_normalized_ms['ms_first_resolution_time_in_minutes_c'])\n",
    "master_results_normalized_ms[\"ms_delta_time_first_reply_solved\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## only keep those which were solved with first reply\n",
    "master_results_normalized_ms_solved_with_first_reply = master_results_normalized_ms[master_results_normalized_ms[\"ms_delta_time_first_reply_solved\"]==0]\n",
    "master_results_normalized_ms_solved_with_first_reply.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all ms related columns\n",
    "master_results_normalized_solved_with_first_reply=master_results_normalized_ms_solved_with_first_reply.drop([col for col in master_results_normalized_ms.columns if col.startswith(\"ms_\")], axis=1) ## drop any metric set related columns\n",
    "master_results_normalized_solved_with_first_reply.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat WordClouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(master_results_normalized_solved_with_first_reply['te_ce_body_tokens_lem_no_stopwords'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud for all emails    \n",
    "    \n",
    "show_wordcloud(master_results_normalized_solved_with_first_reply['te_ce_body_tokens_lem_no_stopwords'], \"All tags\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordclouds by categories\n",
    "\n",
    "word_cloud_for_categories(master_results_normalized_solved_with_first_reply,\"te_ce_body_tokens_lem_no_stopwords\",\"tags_reason_new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ref = client.dataset('zendesk_s2ds_processed')\n",
    "table_ref = dataset_ref.table('master_table_emails_sanitized_body_normalized_first_reply_solved')\n",
    "client.delete_table(table_ref, not_found_ok=True)\n",
    "client.load_table_from_dataframe(master_results_normalized_solved_with_first_reply, table_ref).result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
